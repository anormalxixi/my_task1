# 二轮笔记

## 神经网络概念 流程

机器学习三要素：模型、策略、算法

**公设/策略**：对f（x）函数值进行修饰后判断

**模型**：神经元 每个神经元都有w，b一组系数，有多少个神经元就有多少个分量，激活函数的变量用向量表示了，输出结果的曲面或者分界线可以发生弯曲，复杂度随着神经元的增加而增加，复杂的曲线能被表达出来，近似定理。 神经网络的复杂性来自于激活函数。

把中间层看作是**升维**操作，有多少神经元就会把数据升到多少维，维度够高，一定能够找到一个超平面，能够对数据进行划分。最后一层只用考虑策略问题（间隔最大最好/概率最大最好 sigmoid（二分类）对数据概率化/方差最小最好）多个节点--多分类。

输入层->隐藏层->输出层

神经网络为什么要这么多层？有不同层次的特征，底层的特征回被更加容易地复用。隐藏层地层数越深，对数据特征抽象程度越高

**算法**：反向传播 指向上升最快的向量在参数平面上的投影叫梯度，空间上的是在切平面上的投影，梯度本身是一个向量，说明它在所有维度上都有一个分量，所有的参数都按照梯度的分量进行调整。

还需要步长--学习率，需要手动配置，超参数。

如何求梯度$\nabla f$ ，也就是如何求出这些分量的具体值，对函数求偏导，对每一个维度都要求求偏导

虽然有梯度传到了这里，但是与学习率相乘之后变化很小，不会做出多少调整--**梯度消失**问题

## 中文文本分类

讲文本映射到预先给定的某一类别或某几类别主题的过程，实现这一算法模型叫做分类器

![image-20230411143046483](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230411143046483.png)

### 文本预处理

语义单元--句子、短语、词语、字

中文最小语义单元——一般认为是词语，可以有一个或者多个汉字组成。

#### 分词

````py
import jieba

stop_word_list= pd.read_csv(".txt",index_col=False,quoting=3,sep=,names=[''],encoding='utf-8')
````
#### 去停用词
````py
def txt_cut(sentence):
    lis=[word for word in jieba.lcut(sentence) if w not in stop_word_list.values]#如果停用词是文件且以df读入的话.values
    return " ".join(lis)#把lit里面的各个成员用空格隔开

data['cutword']=df['text'].astype('str').apply(txt_cut)
````

这个函数展开来写应该是str.join(item)，join函数是一个字符串操作函数。str表示字符串（字符），item表示一个成员，注意括号里必须只能有一个成员

pd.read.csv()中quoting=3全部读取

读取txt 如果没有标题行 指定header=None,names=['']添加列名/标题

#### 特征选择

**词袋模型**

词的维度太大，计算困难，文档稀疏；仅仅考虑词语出现的次数，没有考虑句子词语之间的语义信息

**TF-IDF模型**

1. TF：term-frequency词频 

​		体现的是词语在文档内部的重要性，一般是词语出现越多越重要，但是长文档中的词语次数普遍比短文档中的次数多，所以
$$
tf_{ij}=\frac{n_{ij}}{\sum_k n_{kj}}
$$
分子：文档j里面第词i在文档j中出现的频次 分母：文档j中所有词的个数

2.  IDF：inverse document frequency逆文档频率

​		体现的是词语在文档间的重要性，如果某个词语仅出现在极少数的文档中，说明该词语对于文档的区别性强，对应的特征值高
$$
idf_{i}=log(\frac {|D|}{|D_i|+1})
$$
分母：为了避免有部分新的词没有在语料库里出现导致分母为0的情况出现，拉普拉斯平滑。

**TF-IDF 方法的主要思路是一个词在当前类别的重要度与在当前类别内的词频成正比，与所有类别出现的次数成反比。**

相乘得到TF-IDF的值，把每个句子中的每个词的TF-IDF值添加到向量表示出来就是每个句子的TF-IDF特征

## CNN

### Convolution

1. 侦测某个pattern不需要一整个image 只需要一小**部分**
2. 同一个pattern可以出现在image的不同区域
3. subsampled下采样

filter-Matrix-Feature Map 其中的值是network的parameters，挪动的距离stride

处理彩色的image rgb三个颜色来表示一个pixel的话，input是3\*6\*6

和全连接的比较：

把全连接的一些权重拿走，全链接本来一个神经元链接所有的输入，现在连接减少了；而且，同一层的不同神经元对应的weight一样，share weights；

### Max Pooling

根据filter可以获得matrix，把输出四个一组，可以选平均/最大的保留下来

conv->max pooling 得到一个新的image 深度由深度决定 每个filter是一个channel

input有多少个filter output就有多少个层，conv在接收image的时候是会考虑深度的，一次性考虑所有的channel

### Flatten

把feature map拉直 丢进全连接神经网络

### CNN in keras

一个image长宽是一维，是彩色的就是三维

修改structure和输入format 向量->3D tensor

池化操作多的一行或者一列删掉

![image-20230411162835038](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230411162835038.png)

![image-20230411162919039](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230411162919039.png)

拿出第k个filter拿出来，每个元素称$a_{ij}^k$

Degree of the activation of the k-th filter:$a^k=\sum^{11}_{i=1}\sum^{11}_{j=1}a_{ij}^k$

第k个filter有多被activate，多被启动，输入的东西和第k个filter有多相近;我们想知道第k的filter的作用是什么，找一个image--x使得第k个filter 的degree $x*=\mathop{argmax}\limits_x\ a^k$(gradient ascent),现在model的参数固定，更新x，让degree最大 。梯度上升可以找到让filter激活值最大的input，观察这个input就可以知道filter长啥样(对什么特征最敏感)

filter工作 detect不同角度的线条

![image-20230411174005883](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230411174005883.png)

1. embedding
2. 通过a组b种卷积核
3. 最大池化，拼接
4. softmax分类

# **keras**

**类**和**函数**存储在一个单独的文件中，这个.py文件就被称为一个**模块**，为了方便管理而将文件即模块结构化打**包**（\__init__.py），**库**(Lib)是具有相关功能模块和包的集合，分为标准库、第三方库以及自定义模块，**API**（Application Programming Interface）应用程序**接口**，发送请求（应用、数据、设备）-api（传递数据）-接收反馈。keras是高级api。**函数式API**类似于函数的调用，把每一层看成是一个函数来调用这一层，相对于之前只有一个输入，一个输出，中间都是顺序连接的单一结构的模型，函数式api可以处理非线性拓扑结构、具有**共享层的layers**的模型以及**多输入多输出**的模型(categorical_input,numeric_input\binary_pred,categorical_pred)。

## 流程

**构建**模型：model

a.序列式sequential

b.函数式functional

c.子类化subclassing

#### **编译**模型

```py
model.compile(loss='',#损失函数
             optimizer='',#优化方法
             metrics=''#监控指标)
```

#### **拟合**模型

```python
model.fit(X_train,y_train,#训练集
		epochs=,#迭代次数
		batch_size=
		validation_data=(X_valis,y_valid),#验证集
		callbacks=[some_callbacks]#终止模型/保存)
```

#### **评估**模型

```py
model.predict(X_test)#返回的是数值，表示样本属于每一个类别的概率
model.predict_class(X_test)#返回的是类别的索引，即该样本所属的类别标签
model.evaluate(X_test,y_test)#输入数据和标签，输出损失值和指定的指标值--评估
```

**verbose**

+ 在fit和evaluate中都有

是日志显示，有三个参数可以选择 0 1 2

0：不输出日志信息，loss、acc、进度条都不输出

1：带进度条的日志信息

2：为每个epoch输出一行记录，无进度条

+ 训练和评估时都默认取值1，但在评估的时候只有0，1

## 张量tensor

**keras的数据格式--numpy array**

#### 类别

数字的容器，张量是句子任意维度的推广，张量的维度通常叫做轴

**标量**scalar\0D：仅包含一个数字的张量。

可以用ndim属性来查看有一个Numpy张量的轴的个数。`x=np.array(12)` `x.ndim`

**向量**1D:数字组成的数组，一维张量只有一个轴。

`x=np.array([12,3,6,14,7])`注意：这个向量有5个元素，称作5D向量；5D向量只有一个轴，沿着轴有5个元素（沿着轴可以有任意个维度）。维度可以表示某个轴上的元素个数（5D向量），也可以表示张量中轴的个数（5D张量）--准确5阶张量

**矩阵**2D：向量组成的数组（matrix）或二维张量。

**3D张量以及更高维张量**：将多个矩阵2D合成一个新的数组--3D张量--立方体，同理，多个3D->4D张量

#### 关键属性

1. 轴的个数 也叫张量的ndim
2. 形状 一个整数元组，表示张量沿每个轴的维度大小（元素个数），如矩阵(3,5)，3D张量(3,3,5)，向量的形状只包含一个元素(5,),而标量的形状为空()
3. 数据类型 （dtype）这是张量中所包含的数据的类型。

#### 数据张量

| 示例     | ndim   | 形状                                   | 层         |
| -------- | ------ | -------------------------------------- | ---------- |
| 向量数据 | 2D张量 | (samples,features)                     | 全连接层   |
| 时间序列 | 3D     | (samples,timesteps,features)           | 循环层     |
| 图像数据 | 4D     | (samples,height,width,channels)        | 二维卷积层 |
| 视频     | 5D     | (samples,frames,height,width,channels) |            |



## 常见网络层

多个**层**链接在一起组成**模型**，将**输入**数据映射为预测值。然后**损失函数**将这些预测值**输出**，并于目标进行比较，得到损失值用于衡量网络预测值与预期结果的匹配度，**优化器**使用这个损失之来更新网络的权重

#### 嵌入层--embedding层

`keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length)`

将正整数（下标）转化为具有固定大小的向量

输入数据：二维张量（一个批次内的文本数batch size、每篇文本中的词语数）

*input_dim = vocab_size maximum integer index**+1***

输出数据：一个三维张量（同上+每个词语）

global average pooling 1D layer?

flatten 

#### Conv1d

`tf.keras.layers.Conv1D(filters,kernel_size,strides=1)`

input:e.g.(10,128) 10个128维向量的序列 （None,128）任意长度的具有128维向量的序列，

#### MaxPooling1D

`tf.keras.MaxPooling1D(pool_size=2,strides=Node,padding='valid')`

在池化的窗口中选择最大值。

valid意思是不做padding

#### GlobalMaxPooling1D

`tf.keras.GlobalMaxPooling1D(data_format='channels_last')`

pool_size是最长的维度 全局最大池化 没有滑动空间 padding的问题就不存在了

## 网络配置

#### 回凋函数

回调函数可以获得它所联系的模型，是调用fit时传入模型的一个对象，它在训练过程中的不同时间点都会被模型调用，可以访问模型的状态和性能的，还可以采取行动：中断训练、保存模型、加载一组不同的权重或者改变模型的状态

> 当loss不再下降时，如何中断训练 

keras.callbacks.Callback()用来组件新的回调函数的抽象基类

```py
class myCallback(tf.keras.callbacks.Callback)://继承父类
	def on_epoch_end(self,epoch,logs={}):
        if(logs.get('loss')<0.4):
            print("\nLoss is low so cancelling training!")
            self.model.stop_training = True
            
	def on_train_begin(keras.callbacks.Callback):
        def on_train_begin(self,logs={}):
            self.losses = []
            
        def on_batch_end(self,batch,logs={}):
            self.losses.append(logs,get('loss')
```

1. 实例化 `callbacks=myCallback()`
2. 在model.fit中加入`callbacks=[callbacks]`使用回调参数并传递类实例,传递一个列表的回调函数作为callbacks的关键字参数到**Model类型**的**.fit方法**

被回调函数作为参数的**logs字典**，它包含损失指，以及批次或周期结束的所有指标

>保存最佳模型

```py
keras.callbacks.ModelCheckpoint(file_path=,
								verbose=,
								save_weights_only=True,
								save_freq=5*batch_size,
								monitor=''
								save_best_only=,
								)
```



> 记录损失历史

```py
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self,logs={}):
        self.losses=[]
    def on_batch_end(self,batch,logs={}):
        self.losses.append(logs.get('loss'))
       
history=LossHistory()
model.fit(...,callbacks=[history])
print(history.losses)
```



## 预处理功能

### 文本预处理 

preprocessiong.text

#### tokenizer

**文本标记实用类**

 ***import***

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
```

+ 分词器***Tokenizer*** 允许用两种方法向量化一个文本语料库，用于向量化文本，或将文本转换为序列

```python
keras.preprocessing.text.Tokenizer(num_words=None,
#需要保留的最大词数，基于词频;通常不知道有多少个不同的单词，所以设置
								   filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~ ', 
                                   lower=True, 
                                   split=' ', 
                                   char_level=False, 
#是否将每个字符都认为是词，默认是否；在处理中文的时候如果认为每个字都是词，这个参数改为True
                                   oov_token=None, 
 #如果True会添加都词索引中，用来替换超出此表的字符 ='<oov>'表示超纲的词汇！<\UNK>无法识别unknow!
                                   document_count=0)
```
默认情况下，删除所有标点符号，将文本转换为空格分隔的单词序列（单词可能包含 `'` 字符）。 这些序列然后被分割成标记列表。然后它们将被索引或向量化。

`0` 是不会被分配给任何单词的保留索引。

  + 构建分词器

    `form tensorflow.keras.preprocessing.text import Tokenizer`

    ==`tokenizer=Tokenizer(num_words=)`==

+ ***fit_on_texts(texts)*** 适配文本，对文本做了适配之后，只是对词语做了编号和统计，文本并没有全部变为数字。***text_to_sequences***文本序列变数字序列

  + ==`tokenizer.fit_on_texts(cropus)`==

  参数是用于训练的文本列表，返回值：无

  + word_counts 统计词出现次数
  + word_index 给单词编号 ==`word_index=tokenizer.word_index`==返回的是一个字典
  + document_count 处理了几段文本

#### text_to_word_sequence

+ 句子分割***text_to_word_sequence***将一个句子拆分成单词构成的**列表**，输出向量序列 [[单词1,2,3,\][句子2]...]

  + ==`sequences=tokenizer.texts_to_sequences(sentences)`==

  + `vocab_size=len(tokenizer.word_index)+1`词汇总量等于文本中所有词汇＋1，＋的1就是用来填充0的

+ 中文不像英文有空格标志词区分词汇，所以中文要先分词

```
import jieba
sentence = " ".join(jieba.cut("欢迎来北京大学餐厅")) 
print(sentence) # 欢迎 来 北京大学 餐厅
```

确定文本裁剪的长度：即选择max_words

>  把sentence数字化之后，查看文本的长度，画柱状图

```py
x=tok.texts_to_sequences(data['cutword'].values)
length=[]
for i in X:
	length.append(len(i))
v_c=pd.Series(length).value_counts()
c_v[v_c>50].plot(kind='bar',figsize=(12,5))
```

#### one-hot编码

`from tensotflow.keras.utlis import to_categorical `

***to_categorical()***

### 序列预处理

prepocessing.sequence

#### **pad_sequences**

序列填充--让序列长度统一

+ `from tensorflow.keras.preprocessing.sequence import pad_sequences`
+ 以序列中最长的那一条为标准，其他短的数据会在前面补上0（==padding='pre/post'==）
+ 裁剪成固定的长度==maxlen\===、==truncating\= ==
  + `padded=pad_sequences(sequences，maxlen=5,padding='post',truncating='post')-->表示从后部截断/pre`

## 构建模型

#### 序列式

线性堆叠

`Sequential()`创建一个空模型

`add()`函数一层层加layers

**参数**

+ 每个层的第一个参数都是**设定该层输出数据的维度**
+ Dense层的第二个参数设定了**激活函数**的方式

`model.summary()`概要信息

`model.layers`打印层的信息

`model.layers[n].name`

`W,b=model.layers[n].get_weights()`

输出形状(None,784)--None是一批batch里面的样本数，为了代码简洁，这个0维--（点）的样本数在建模时不需要显性写出来

#### 函数式建模

+ 需要多个输入
+ 需要多个输出
+ 在层与层之间有内部分支

`Input`

`Model`

把层当作函数用，一层层把input传递到output，最后再用Model（）建立关系

#### 子类化建模

+ `__init__()`
  + 创建层并设置为类实例的属性
+ `call（）`
  + 定义前向传播,负责各种计算，有个参数input
+ `compute_output_shape()`
  + 计算模型输出的形状



> 在每个epoch后记录训练/测试的loss和正确率

model.fit在运行结束后返回一个history对象，其中包含了训练过程中的损失函数的值以及其他度量指标

```
hist = model.fit(X,y,validation_split=0.2)//切分数据作验证
print(hist.history)
```

> 验证模型准确度

```
scores = model.evaluate(testx,testy,verbose=1)
## model.metrics_names-->evaluate默认输出结果时验证集的损失值loss和准确率accuracy
```

## 保存模型

#### CheckPoint

```py
callbacks_list = [
    keras.callbacks.EarlyStopping(
    	monitor='acc',#被监测模型精度
    	patience=1#没有进步的训练轮数为1，在着之后就会停止训练
    ),
    keras.callbacks.ModelCheckpoint(
    	filepath='path_to_my_tf_checkpoint',
        monitor='val_loss',#如果没有改善就不覆盖
        save_best_only=True
    )
]
new_model.load_weights(;path_to_my_tf_checkpoint)
```

print("Restored model,accracy:{5.2f}%".format(100*acc))

#### SavedModel格式

```py
model.save('',save_format='tf')#文件夹--网络结构、权重、配置、优化器状态
new_model = keras.models.load_model('')
```

#### SubClass Model

通过继承Model类来实现的模块子类化，只能整个模型保存成SavedModel模型，将权重保存成checkpoint格式的权重

#### 保存权重

`model.save_weights`

### 超参数

vocab_size=
embedding_dim=
max_length=
trunc_type=
ovv_tok='<oov>'
training_size=

输入层的词向量表征

卷积核大小、数量

激活函数

池化层策略

正则化选择

![image-20230416180254641](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230416180254641.png)

```
reverse_word_index=dict{[(value,key)]for (key,value)in word_index.items()}

def decode_review(text):
	return ''.join([reverse_word_index.get(i,'?')for i in text])
```

![image-20230416180649033](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230416180649033.png)

```py
#画图
history是model.fit的返回值
def plot_graphs(history,string):
	plt.plot(history.history[string])
	plt.plot(history.history['val_'+string])
	plt.xlabel('Epochs')
	plt.ylabel(string)
	plt.legrnd([string,'val_'+string])
	plt.show()
plot_graphs(history,"acc")
plt_graphs(history,"loss")
```

## 输入形状

None告诉tf在这个维度上 可以接收任何长度的数据 一般用于batch_size的大小

+ 如果输入形状只有一个尺寸，则不需要将其作为元组给出，而将其作为`input_dim`标量数字给出
  + 假设输入层包含3个元素
    + `input_shape(3,)`-只有一维的时候必须用逗号  = `batch_input=(batch_size,3)`
    + `input_dim=3`

# 自定义层

实现自定义层的最佳方法是拓展tf.keras.layers.Layer类并实现

层封装了状态（层的权重）和输入到输出的转换（调用，层的前向传播）

+ `__init__`可以在其进行所有与输入无关的初始化，定义相关的层
+ `build(self.input_shape)`直到输入张量的形状并可以进行其余的初始化
  + 很多情况下事先不知道输入的大小，希望得知该值时再延迟创建权重

+ `call`在这里进行前向传播

如果要用`model.save`保存模型，或者载入模型图中需要用：

重写get_config方法，将位置参数以字典的方式传入

+ `get_config`将返回一个包含模型配置的python字典
+ `from_config`重建模型

`Layer` 类对应于我们在文献中所称的“层”（如“卷积层”或“循环层”）或“块”（如“ResNet 块”或“Inception 块”）。

同时，`Model` 类对应于文献中所称的“模型”（如“深度学习模型”）或“网络”（如“深度神经网络”）。

因此，如果您想知道“我应该用 `Layer` 类还是 `Model` 类？”，请问自己：我是否需要在它上面调用 `fit()`？我是否需要在它上面调用 `save()`？如果是，则使用 `Model`。如果不是（要么因为您的类只是更大系统中的一个块，要么因为您正在自己编写训练和保存代码），则使用 `Layer`。

- 您要训练的外部容器是 `Model`。`Model` 就像 `Layer`，但是添加了训练和序列化实用工具。

- 层可以递归嵌套以创建新的更大的计算块。
- 层可以通过 `add_loss()` 和 `add_metric()` 创建并跟踪损失（通常是正则化损失）以及指标。？

## python查漏

##### map

把数据逐个当作参数传入到字典或函数中，得到映射后的值

```
利用字典
data['label']=data['label'].map(dic).values? map({“男”：0，“女”：1})
```

```
利用函数
def gender_map(x):
	gender =1 if x== '男'else 0
	return gender

data['gender']=data['gender'].map(gender_map)#是函数名
```

##### copy

对数据集进行独立编码的话，保存原始测试集y_test，方便作评价

浅copy一维列表不嵌套的话，一个改变不影响另一个

使用deepcopy()复制列表lis_之后，直接改变二维列表中的值 d,不会影响到源列表lis_D

##### super（）

的目的时让子类调用父类的方法\__init__(),使其包含父类的所有属性，因为父类的init含self

\__init__在初始化一个对象时，定义这个对象的初始值

当子类不做初始化的时候，会自动继承父类的属性；
当子类做初始化（子类中包含新的属性）的时候，子类不会自动继承父类的属性；
当子类做初始化（子类中包含新的属性）的时候，如果子类调用super初始化了父类的构造函数，那么子类会继承父类的属性。

##### split

参数maxsplit 只分割一次

##### **kwargs

keyword arguments关键字参数 将一个可变的关键字参数的**字典**传给函数实参

*--解包

问：正则化加在哪个层都可以吗

add_loss和add_metrics方法和fit返回是一样的吗？

concatenate和Concatenate--在初始化时就要有inputs参数

在keras中，如果不设置batch_size参数，则默认设置为32。

# Gradio

**Interface**

1. 写一个函数 参数由用户输入
2. 实例化一个`gr.Interface`,传入关键字参数有`fn=` `inputs=` `outputs=`(eg:"text","image","audio","label","checkbox",gr.Slider(0,100),gr.Image(shape=(,)/type='filepath')，"number")

3. 组件的属性--可以自定义，不用像上面的字符串快捷方式`inputs=gr.Textbox(lines=2,placeholder='Name here...')`即用一个实类
4. 多输入和多输出组件 `inputs=[,]`输入/出用一个列表把组件包裹起来，输入列表对应的式函数的参数，输出列表对应的是return的值
5. `gr.Radio("add","subtract")`

**Bolck**

1. `with gr.Blocks as demo`

2. `btn=gr.Button('按钮上的字')`实例化一个按钮对象，给按钮添加点击事件`btn.click(fn,inputs,outputs)`

   `gr.Markdown('')`显示文字

   `gr.Tab('')`标签页

   `gr.Accordion('sentence')：`折叠块

3. `with gr.Row():下面跟想要横排列的组件`with

**其他功能**

1. 给出输入例子`examples=[]`
2. 出错提示`gr.Errors`
3. 描述性文本
+ interface constructor
    + `title`
    + `description`
    + `article`
+ Bolcks components
	+ `gr.HTML(...)`
	+ `gr.Markdown(...)`
+ keword argument
	+ `label=`
	+ `info=`

4. 样式风格

+ interface constructor
  + `demo=gr.Interface(...,theme=gr.themes.Monochrome())`
  + Soft()紫白Monochrome()黑白Glass()

+ component
  + `gr.Image(".jpg").style(height='24',rounded=False)` 