# 考核一

## 回归

多元回归模型：
$$
f_{w,b}(x^{i})=W\cdot x^{i}+b
$$

### 最小二乘法

Loss function:损失函数
$$
L(w)=\sum_{i=1}^N||w^\text Tx_i-y_i||^2\\
\hat w=(X^TX)^{-1}XY
$$

### 梯度下降法

cost function:
$$
J(W,b)=\frac{1}{2m}\sum_{i=0}^{m-1}(f_{w,b}(x^{i}-y^{i})^2)
$$

```python
def compute_cost(X,y,w,b):
    """
    X:shape(m,n)
    w:shape(n)
    """
    m=X.shape[0] 样本数量
    cost = 0.0 初始化
    for i in range(m):
        f_wb_i = np.dot(X[i],w)+b
        cost = cost + (f_wb_i-y[i])**2
    cost/=2
    return (np.squeeze(cost))//从数组中删除维度为1的维度
```

$$
repeat:
\begin{align}
	w_j&= w_j-\alpha\frac{\partial J(W,b)}{\partial w_j}\ for\ j =0,1,...n-1\\
	b& = b-\alpha \frac {\partial J(W,b)}{\partial b}
\end{align}
$$

$$
\begin{align}
\frac {\partial J(W,b)}{\partial w_j}& = \frac {1}{m}\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}_j\\
\frac {\partial J(W,b)}{\partial b} &= \frac {1}{m}\sum_{i=0}^{m-1}(f_{w,b}(x^{(i)})-y^{(i)})
\end{align}
$$

```py
def compute_gradient(X,y,w,b):
    """
    X(m,n)
    y(m,)
    w(n,)
    """
    m,n=X.shape
    dj_dw=np.zeros((n,))
    dj_db=0
    
    for i in range(m):
        err = (np.dot(X[i],w)+b)-y[i]//第一个样本的误差
        for j in range(n):
            dj_dw = dj_dw + err*X[i,j]!!获取矩阵元素
        dj_db = dj_db+err
    dj_dw = dj_dw/m
    dj_db = dj_db/m
    
    return dj_db,dj_dw
```

```py
def gradient_descent(X,y,w_in,b_in,cost_function,gradient_function,alpha,num_iters):
	"""
	X(m,n)
	y(m,)
	w_in(n,)
	b_in
	"""
    m=len(X)
    J_history = []
    w=copy.deepcopy(w_in)?
    b=b_in
    
    for i in range(num_iters):
        dj_db,dj_dw=cost_gradient(X,y,w,b)
        
		if i%math.ceil(num_iters/10)==0：//向上取整 灵活获取cost
  		print(f"Iteration {i:4d}：cost{J_history[-1]:8.2f}")//{content:format}//4 字段的最小占位宽度
```

```
initial_w = np.zeros_like(w_init)
initial_b = 0
itterations=1000
alpha = 5.0e-7
w_final,b_final,J_hist = gradient_decent(X_train,y_train,initial_w,initial_b,comput_cost,compute_gradient)

print(f"b,w found by gradient decent:{b_final:0.2f},{w_final}")
m,_ = X_train.shape
for i in range(m):
	print(f"predition:{np.dot(X_train[i],w_final)+b_final:0.2f},target value:{y_train[i]}"")
```



### 正则化

正常是$N\gg p$

原因：可是现实中可能样本没那么多，维度很大，$x^ Tx$的逆可能不存在，数据少特征多容易造成过拟合

方法：

1. 加数据
2. 特征提取pca？/选择 剔除掉不太相关的样本
3. 正则化 对参数空间w的一个约束

框架：
$$
argmin[L(W)+\lambda p(w)]
$$
penalty惩罚项

L1:
$$
Lasso p(w)=||w|| 关于w的一范数
$$
L2:
$$
Ridge 岭回归 p(w)=||w||_2^2=w^\text Tw
$$
L2岭回归又叫**权值衰减**

当$p(w)$是L2一个范数的时候，对优化函数加二范式，求一下w是多少
$$
\begin{align}
J(W)&=\sum_{i=1}^N||W^\text TX_i-y_i||+\lambda W^\text TW\\
&=W^\text T(X^\text TX+\lambda I)W-2W^\text TX^\text TY+Y^\text TY
\end{align}
$$

$$
\hat w=(X^TX+\lambda I)^{-1}X^TY
$$

岭回归情况下所得的解析解w 多了一个单位对角矩阵*lambda->镇定了，抑制了过拟合

噪声符合零均值的正态分布

最小似然估计LSE$\longleftrightarrow$ 极大似然估计MLE（maximum likelihood estimate）

regularized LSE$\longleftrightarrow$ 最大后验估计MAP(noise服从高斯分布)先验也是服从高斯分布

**均值归一化**
$$
x_i=\frac {x_i-\mu_i}{x_{max}-x_{min}}
$$
代价函数随迭代步数增加的变化曲线

为了找$\alpha$尝试，0.001，0.003, 0.01，0.1，1...

可以自己构造、定义一个新的特征

选择特征

多项式函数

正规方程

![image-20230318114130139](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230318114130139.png)

比较模型效能数值结果时，你只能拿不同的模型，在**同样的测试集**上面比。

**matrix**仅能表示二维矩阵，即使输入的是一个一维列表，也会把它强制转为二维矩阵。

**array**不仅能表示二维数组，还能表示1、3、4、5...n维，二维数组也可以叫做矩阵。

使用shape()函数可以得到数组（或矩阵）的行数和列数(m,n)

数组或矩阵都可以做行列互换，称为转置，用**.T**或者**transpose()**函数来实现。

xrange和range区别

## softmax回归

多分类问题

将输入特征和权重做线性叠加，再通过softmax函数进行运算，指数运算，将输出值o1,o2,o3映射到（0，1），看成概率来理解，得到概率分布， y都是正值且相加等于1，从而进行多分类

输出是离散值，由线性回归中的一个变成了多个。

输出i是预测为第i类的置信度

### softmax

一个样本的特征时一个向量：X=x1,x2,x3,...,xn

每一个特征在计算过程中都有一个权重，引入权重参数建立权重结构/直线

g(x)=w0+w1x1+w2x2+...+wnxn=wjxj x0=1

作用：计算一组数值中每个值的占比
$$
P(S_i)=\frac{e^{gi}}{\sum_k^ne^{g_k}}
$$
设有n个分类，i表示k中的某个分类，gi表示该分类的值

矩阵表述
$$
softmax(X)=\frac {e^{XW.T}}{w^{XW.T}E}
$$
步骤：

1. 计算权重指数

o:样本数  n:特征数 m:类别数

X:(o*n)

w:(m*n)

2. 计算分母：S
3. 归一化：Q=softmax=G/S

### 信息量

**事件出现概率小，信息量大**。则信息量的计算因该是单调递减的函数

一个事件出现概率本来是很低的，但是它出现了，则说明这个事件的信息量很大
$$
信息量：=-log_2x
$$

$$
H=-\sum_{i=1}^Mp_ilogp_i
$$

### 熵entropy

无损编码事件信息的最小平均编码长度；信息的混乱程度，不确定性

熵对整体的概率模型进行了一个衡量，结果反映出这个概率模型的一个不确定程度

如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。

随机做一件事，被猜到的难度有多大
$$
H(P):=E(P_f)
$$


一个系统的值，乘上它的占比，然后把所有情况都加起来——期望。

对系统求期望，就是信息量的期望

系统熵的定义：把里面所有可能发生的事件，信息量求出来和这个事件发生的概率相乘，最后把所有的事件都加起来，得到的就是系统的熵



**信息熵是信息量的加权平均**，就是整个事件出现的情况的期望

信息熵描述的是**整个事件**本身的混乱程度（把握这个事件的难易程度），单个信息量贡献给整个系统，要乘以自己的比例才行

信息量是描述一个事件的一种情况出现的难易程度

信息熵：
$$
H=-\sum_{i=1}^Mp_ilogp_i
$$

### 交叉熵cross_entropy

如果我们不知道事件的概率分布，又想计算熵
$$
CrossEntropy:\sum _i^Np_i[-logq_i]
$$

- 计算期望的概率分布是Q，与真实的概率分布P不同。
- 计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。

p表示每一个事物的真实概率

q表示对应的估计概率

随着**预测越来越准确，交叉熵的值越来越小**
$$
L=\frac 1 o[Plog(QT)]^DE
$$
P:样本的真实分类 编码

q:softmax计算结果

均方损失

+ 对类别进行**一位有效**编码

  + $$
    y=[y_1,y_2,...,y_n]^T
    $$

  + 假设真实类别是第i个的话，那么yi=1,其他的是0

+ 最大值最为预测

  + $$
    \hat y=argmaxO_i
    $$

  + 选取i使得最大化o_i的置信度的值作为预测

校验比例

对正确类别的置信度特别大，对正确类y他的置信度远远大于其他非正确类的o_i

需要更置信的识别正确类（大余量）

置 信度高的类别与置信度低的类别差距越大越好
$$
O_y-O_i\ge\Delta(y,i)
$$

+ 输出匹配**概率**（非负，和为1）

  + $$
    \hat y=softmax(\vec o)\\
    \hat y_i=\frac {exp(o_i)}{\sum exp(o_k)}
    $$

  + 输出是o1,o3,...o_n,$\hat y$是长为n的向量，有属性，每个元素非负，和为1

  + y的第i个元素，等于o里面的第i个元素做指数（不管什么值都变成正的），除以所有的O_k做指数

+ 概率y和$\hat y$的区别作为损失

### 梯度下降

$$
Loss=-\frac1 o \sum\sum p lnq
$$

dL_dw:

repeat{

wij=wij+adL_dw

}

W=W+$\alpha$D 

D=(P-Q).T X

#### 梯度检验

双边误差
$$
f'(\theta)=\lim\limits _{\varepsilon\rightarrow0}\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}
$$

#### 交叉验证

training\cross validation dev set\test set=6:2:2

## 决策树

根节点 决策节点 叶子节点

步骤

1. 在根节点的特征，在每个节点上使用哪些功能进行拆分,能够减少杂质，最大化纯度

   1. 加权平均

   2. 熵的减少称为**信息增益**

      information gain

   $$
   H(p_1^{root})-(w^{left}H(p_1^{left})+w^{right}H(p_1^{right}))
   $$

   

2. 什么时候减少分裂，树小，避免过拟合

3. 最大决策深度

ID3的核心思想：通过比较信息增益的大小，确定特征的询问决策顺序，信息增益越大，就越靠前询问决策分类

图：

retrieve：检索（存储的信息）

- frameon:是否显示边框

## 朴素贝叶斯

#### 条件概率

$$
p(c_i|x,y)=\frac {p(x,y|c_i)p(c_i)}{p(x,y)}
$$

c:表示类别，输入待判断数据，式子给出要解的某一类的概率，最终的目的式比较各个类别的概率值大小。贝叶斯决策的核心思想：选择具有最高概率的决策

假设X的特征之间是**独立**的：
$$
p(x,y|C_0)=p(x_0,y_0|C_0)+...+p(x_n,y_n|C_0)
$$

#### 分类器

##### 先验概率P(X)

根据以往的经验和分析得到的概率

##### 后验概率P(Y|X)

事情X已经发生的前提下，事件Y发生的概率，称事件X发生下事件Y的后验概率

**拉普拉斯修正**

若某个属性在训练集中没有与某个类同时出现过，则训练后的模型会出现over-fitting现象。若训练集没有该样例，连乘计算的时候概率值为0，样本不存在，不代表该事件一定不可能发生。为了避免其他属性值携带的信息，被训练集中未出现的属性值“抹去”，在估计概率值的时候通常要进行修正:(N表示训练集中可能的类别数，N_i表示第i个属性可能的取值)
$$
P(C)=\frac {|D_c|+1}{|D|+N}\\
P(x_i|c)=\frac {|Dc,x_i|+1}{|D|+N_i}
$$

#### 防溢出

原因：条件概率乘法计算时，因子比较小。当属性数量增加的时候，会导致累乘结果下溢出的现象

方法：取对数，把连乘转化乘对数累加。分类结果仅需要对比概率的对数累加运算后的数值，确定划分的类别

#### 词集模型

对于给定文档，只统计某个词汇是否在文本档中出现

#### 词袋模型

统计词汇时往往需要剔除重要性极低的高频词和停用词

#### 数据预处理

1. 词典：囊括必要词汇
2. 文本向量化。每个文档都定义为词典大小，分别遍历某类文档中的词汇并统计出现次数，最后得到一个和词典一样大小的向量，这些向量有一个个整数组成，每个整数代表了词典上对应位置词汇在当下文档中出现的频率



## 问题与解决

+ python里np.array 的shape(n,)是一维数组，里面有n个元素
  shape(n,1)是二维数组，n行1列
+ softmax回归里面 指数运算overflow，减去最大值后又underflow了；
  + 最后，每个元素除以255
+ **append**添加是将容器看作整体来进行添加，但**extend**是将容器打碎后添加（加入的只是元素）
+ 读取txt文件，对英文单词的处理，标点符号，变形
+ 去掉符号的时候，返回值是新值，改变了字符，但是原来的dataset没有改变，只能在读取文件的时候先去掉标点，再分割了

#### 最大似然估计法

原理：利用已知的样本，找到最有可能生成该样本的参数

##### 似然函数

统计模型中参数的函数，表示模型参数的似然性

**对同一个似然函数，其所代表的模型中，某项参数值具有多种可能，但如果存在一个参数值，使得概似函数值达到最大的话，那么这个值就是该项参数最为“合理”的参数值。**

+ 似然性：likelihood L($\theta$|x) 是在已知观测结果x出发，分布函数的参数$\theta$的可能性大小

+ 概率：possibility P(x|$\theta$)是在已知参数$\theta$的情况下啊，发生观测结果x的大小;

##### 最大似然估计MLE

核心思想：对于给定的观测数据x，我们希望能从所有的参数 中找到能最大概率生成观测数据的参数$\theta$ 作为估计结果

在实际运算中，我们讲待估计的参数$\theta$ 看成变量，计算得到生成数据x的概率函数p（x|$\theta$），并找到能最大化概率函数的参数即可
$$
\theta ^*= argmax_\theta p(x|\theta)
$$
**初始化参数**一般很小

np.random,randn((2,2))*0.01 防止梯度消失

方差(train)：high variance 欠拟合

偏差（dev）：high bias 过拟合

### 数据预处理

在将数据放入模型或算法前，需要根据数据的特征及任务要求对数据进行预处理。尝试采用以下几种数据预处理方法（有需要就用）：

- 归一化和标准化
- 独热编码
- 连续值离散化处理
- 缺失值的处理
- 异常值的检测和处理
- 分割数据集
- ...

### 模型评估

不同任务及模型都有其对应的评估指标，例如MSE，Accuracy，f1，recall......

对于实现的模型，找到相应的指标对其进行评估，在必要时可以进行**可视化**操作

### 爬虫

#### HTTP请求和响应

Hypertext Transfer Protocol超文本传输协议

客户与用户之间的请求响应协议

Python Request库——创建和发送请求

<img src="C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230321180012541.png" alt="image-20230321180012541" style="zoom:50%;" />

状态行：协议版本/状态码/状态消息

![image-20230321174722323](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230321174722323.png)

响应头：包含告知客户端的信息（data、content-type返回内容的类型及编码格式）

响应体：服务器想给客户端的数据内容

![image-20230321185732441](C:\Users\陈欣禧\AppData\Roaming\Typora\typora-user-images\image-20230321185732441.png)

多线程：threading让不同线程同时爬取多个页面
